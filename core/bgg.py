import os
from pathlib import Path
import json
import random
import logging
import sys
from copy import deepcopy
from datetime import datetime
from time import sleep, time
from statistics import median
from typing import Union
import requests


class Retriever:
    """Class for handling board game data retrieval.

    Initialize with a directory path for saving data.
    Call Retriever.retrieve_all to get all board game data.
    Retriever.api_request and Retriever.generate_game_uri are provided as lower
    level methods.
    Retriever.remove_progress_file is a convenience method for deleting
    progress files generated by Retriever.retrieve_all.
    """
    # Approximate highest ID on BGG website.
    # Get by probing https://boardgamegeek.com/boardgameaccessory/<id>/
    # until you have problems.
    MAX_ID = 362383
    # Possible values progress objects
    PROGRESS_STATUS_COMPLETE = 'complete'
    PROGRESS_STATUS_INCOMPLETE = 'incomplete'
    PROGRESS_STATUS_QUEUED = 'queued'
    # Field keys for individual batches in progress objects
    PROGRESS_KEY_IDS = "ids"
    PROGRESS_KEY_STATUS = "status"
    PROGRESS_KEY_LAST_ACCESSED = "last_accessed"
    # BGG API root
    BASE_API = "https://boardgamegeek.com/xmlapi2/thing?"
    # Save directory sub folders/file strings
    PATH_XML_DIR = 'xml'
    PATH_PROGRESS_FILE = 'progress.json'
    PATH_LOG_FILE = 'retriever.log'
    # Wait time before trying again if there's no internet
    PAUSE_TIME_NO_CONNECTION = 60

    def __init__(self, save_dir: str) -> None:
        """Initialize Retriever with a dir for saving data.

        Args:
            save_dir (str): directory path for saving downloaded data,
                log files, progress tracking files, etc.

        Raises:
            FileNotFoundError: if save_dir doesn't exist
            NotADirectoryError: if save_dir isn't a directory
        """
        save_dir = Path(save_dir)
        if not save_dir.exists():
            raise FileNotFoundError(f"Dir {str(save_dir)} does not exist.")
        if not save_dir.is_dir():
            raise NotADirectoryError(f"{str(save_dir)} is not a directory.")
        # Set up subdirectories/files in save dir
        xml_dir = save_dir / self.PATH_XML_DIR
        xml_dir.mkdir(exist_ok=True)
        self.xml_dir = str(xml_dir)
        progress_path = save_dir / self.PATH_PROGRESS_FILE
        self.progress_path = str(progress_path)
        log_file_path = save_dir / self.PATH_LOG_FILE
        self.log_file_path = str(log_file_path)

    def retrieve_all(
            self,
            batch_cooldown: int = 10*60,
            server_cooldown: int = 12*60*60,
            batch_size: int = 1000,
            shuffle: bool = True,
            random_seed: int = None,
            max_id: int = None) -> None:
        """Retrieve all board games from Board Game Geek.

        By default, gets board games in randomized batches.
        As the BGG ids (from 1 to ~362383) are not evenly distributed
        in terms of actual board games vs board game accessories,
        randomization allows the user to have a representative sample
        of board game ids should they choose to terminate the operation early.

        Cooldown periods between batches help prevent server overloading.
        As well a longer cooldown period can be applied should the user
        encounter a server error. These errors could be a result of getting
        blocked by the server for too many requests, or the server being down
        due to maintenance. This generic strategy is used as the server error
        codes/reasons do not appear to be publicly documented.

        A batch request might get a 200 response, 202 response, or other
        responses. 200 response is a successful batch request. 202 indicates
        that the server has queued the request for later processing. Of the
        other responses, there is at least a 502 response that includes a
        message saying that there is a server error and that you can try again
        in 30 seconds. In this 502 case, the longer cooldown period is skipped,
        and the next batch is requested.

        200, 202, and other responses are marked in a 'progress.json' file as
        'complete', 'queued', and 'incomplete', respectively. The number of
        batches of each status (as well as retrieval run events) is logged to
        'retriever.log'. If the number of 'queued' and 'incomplete' statuses is
        not zero, running retrieve_all with a Retriever object instantiated
        with the same save_dir will load that 'progress.json' file and request
        only the unfinished batches.

        References:
        1) https://boardgamegeek.com/wiki/page/BGG_XML_API2

        Args:
            batch_cooldown (int, optional): Seconds to cooldown between
                batches. Defaults to 10*60, i.e. 10 min.
            server_cooldown (int, optional): Seconds to cooldown on
                encountering a server response error code. Defaults to
                12*60*60, i.e. 12 hours.
            batch_size (int, optional): Number of 'thing' ids to request in one
                batch. Defaults to 1000. Note: 1000 is a conservative
                estimate of where the server might block the request for being
                large, when requesting board games only (i.e. not expansions
                and accessories). Board games comprise only about a third of
                all ids.
            shuffle (bool, optional): Whether ids should be requested in a
                randomized order. Defaults to True.
            random_seed (int, optional): Seed for randomizing order, supplied
                to random.seed(). Defaults to None.
            max_id (int, optional): Provide a max_id to download up to,
                otherwise uses preset self.MAX_ID.
        """
        # Set max_id if not provided
        if max_id is None:
            max_id = self.MAX_ID

        # RetrieverLogger is a helper class that tracks
        # relevant retrieval statistics while logging.
        log = RetrieverLogger(self.log_file_path)
        log.log_run_start()
        # Resume from an existing progress file
        # or create new progress object and batches.
        if self._check_progress_file_exists():
            log.log_resuming_from_file()
            progress = self._load_progress_file()
        else:
            log.log_new_progress_file()
            ids = [i for i in range(1, max_id + 1)]
            if shuffle:
                random.seed(random_seed)
                random.shuffle(ids)
            progress = self._create_progress_object(ids, batch_size=batch_size)
            self._save_progress_file(progress)  # Initial save

        # Loop progress object, ignoring already complete batches.
        # Defensively deepcopy since we're mutating during iteration.
        log.log_total_batches(progress)
        for idx, batch in enumerate(deepcopy(progress)):
            if batch[self.PROGRESS_KEY_STATUS] == \
                    self.PROGRESS_STATUS_COMPLETE:
                continue
            else:
                # Try the request, but pause if no internet
                while True:
                    try:
                        log.log_batch_start(idx)
                        uri = self.generate_game_uri(
                            batch[self.PROGRESS_KEY_IDS]
                            )
                        r = self.api_request(uri)
                        break
                    except requests.ConnectionError:
                        print(f"Unable to connect to internet, "
                              f"pausing {self.PAUSE_TIME_NO_CONNECTION}"
                              f" seconds.")
                        self._countdown(self.PAUSE_TIME_NO_CONNECTION)
                        continue
                # First, no matter the result, save the access time
                batch[self.PROGRESS_KEY_LAST_ACCESSED] = \
                    datetime.now().strftime('%Y-%b-%d %H:%M:%S.%f')
                # If its 200, save the file, change status to complete
                # If it's 202, mark it as queued.
                # Anything else, could mean server blocking or down,
                # so wait a while, then try again.
                if r.status_code == 200:
                    batch[self.PROGRESS_KEY_STATUS] = \
                        self.PROGRESS_STATUS_COMPLETE
                    progress[idx] = batch
                    self._write_response(r, self.xml_dir + f'/{idx}.xml')
                    self._save_progress_file(progress)
                    log.log_batch_downloaded(idx, r, batch_cooldown)
                elif r.status_code == 202:
                    batch[self.PROGRESS_KEY_STATUS] = \
                        self.PROGRESS_STATUS_QUEUED
                    progress[idx] = batch
                    self._save_progress_file(progress)
                    log.log_batch_queued(idx)
                else:
                    will_cooldown = True
                    # There is a 502 condition where the server error
                    # recommends trying again in 30 seconds.
                    # In that case, skip long server cooldown,
                    # but move onto the next batch.
                    if (r.status_code == 502) and \
                       (r.text.find("try again in 30 seconds") != 1):
                        will_cooldown = False
                    log.log_batch_error(idx, r)

                    batch[self.PROGRESS_KEY_STATUS] = \
                        self.PROGRESS_STATUS_INCOMPLETE
                    progress[idx] = batch
                    self._save_progress_file(progress)

                    # For all other error codes,
                    # cooldown a longer time in case it means
                    # the server is doing some sort of blocking
                    # without explicitly notifying us.
                    if will_cooldown:
                        log.log_cooldown_start(server_cooldown, 'server')
                        self._countdown(server_cooldown)
            # Cooldown between batches to not overload
            # or get blocked by server.
            log.log_cooldown_start(batch_cooldown, 'batch')
            self._countdown(batch_cooldown)

        # End of run logging
        log.log_run_complete_summary(
            progress,
            self.PROGRESS_KEY_STATUS,
            [self.PROGRESS_STATUS_COMPLETE,
             self.PROGRESS_STATUS_QUEUED,
             self.PROGRESS_STATUS_INCOMPLETE]
            )

    def api_request(self, uri: str) -> requests.Response:
        """Make a request for board game geek data.

        Args:
            uri (str): String URI for accessing the API.

        Returns:
            requests.models.Response: response from the HTTP request.
        """
        r = requests.get(uri)
        return r

    def generate_game_uri(
        self,
        ids: list = None,
        filter_basegame: bool = True,
        filter_expansion: bool = False,
        stats: bool = True,
        base_api_endpoint: str = BASE_API
    ) -> str:
        """Construct a URI to get one or more games based on their id.

        Args:
            ids (list, optional): numerical ids of the board games.
                Defaults to None.
            filter_basegame (bool, optional): filter for base board games.
                Defaults to True.
            filter_expansion (bool, optional): filter for
                board game expansions. Defaults to False.
            stats (bool, optional): include statistics
                i.e. "stats=1" in the query. Defaults to True.
            base_api_endpoint (str, optional): Board Game Geek XML API2
                endpoint for retrieving "thing" objects. Defaults to BASE_API.

        Returns:
            str:  URI accessing board games with specified ids.
        """
        # Thing types
        BASE_GAME_TYPE = "boardgame"  # i.e. non expansions
        EXPANSION_TYPE = "boardgameexpansion"

        # Instantiated empty list
        if ids is None:
            ids = []

        # Set up the output
        uri = base_api_endpoint

        # Add type filters
        uri += "type="
        if filter_basegame:
            uri += BASE_GAME_TYPE + ','
        if filter_expansion:
            uri += EXPANSION_TYPE + ','
        uri += '&'

        # Add stats query
        if stats:
            uri += "stats=1&"

        # Add ids
        uri += "id="
        # i.e. [1,2] -> '1,2'
        uri += ','.join([str(i) for i in ids])
        return uri

    def remove_progress_file(self) -> None:
        """Deletes the progress file at the save path.

        Does not error if file is missing.
        """
        Path(self.progress_path).unlink(missing_ok=True)

    def _create_progress_object(
            self,
            ids: list,
            batch_size: int = 1000) -> list:
        """Batchify list of ids, returning progress object with statuses per batch.

        Args:
            ids (list): BGG thing ids e.g. board games.
            batch_size (int, optional): Defaults to 1000.

        Returns:
            list: of dicts containing batches of ids and status info
        """

        progress = [
            {self.PROGRESS_KEY_IDS: ids[i: i+batch_size],
             self.PROGRESS_KEY_STATUS: self.PROGRESS_STATUS_INCOMPLETE,
             self.PROGRESS_KEY_LAST_ACCESSED: ''}
            for i in range(0, len(ids), batch_size)]

        return progress

    def _save_progress_file(self, progress: list) -> None:
        """Saves a progress object to self's save path."""
        with open(self.progress_path, 'w') as f:
            json.dump(progress, f, indent=4)

    def _load_progress_file(self) -> dict:
        """Returns a dict from 'progress.json' file in self's save path."""
        with open(self.progress_path, 'r') as f:
            progress = json.load(f)
        return progress

    def _check_progress_file_exists(self) -> bool:
        """True if 'progress.json' file already exists."""
        return os.path.isfile(self.progress_path)

    def _countdown(self, time_to_sleep: int) -> None:
        """Prints a countdown timer.

        Args:
            time_to_sleep (int): in seconds.
        """
        # Defensive int coercion
        time_to_sleep = int(time_to_sleep)
        for i in range(time_to_sleep, 0, -1):
            h = i // 3600
            m = (i % 3600) // 60
            s = i % 60
            # Countdown in place without new lines
            print(f"\rResuming in {h:02}h {m:02}m {s:02}s", end='')
            sleep(1)
        print(f"\rResuming in {0:02}h {0:02}m {0:02}s")

    def _write_response(
            self,
            response: requests.Response,
            out_path: str) -> None:
        """Write the content of a response to a file.

        Args:
            response (requests.Response): Response object
            out_path (str): location to write the file
        """
        with open(out_path, 'wb') as f:
            f.write(response.content)


class RetrieverLogger:
    """Convenience class for logging from Retriever.

    Instantiate in `Retriever.retrieve_all` or related method calls.
    Use RetrieverLogger.log_run_start call to assign self.time_start.
    """
    def __init__(self, log_file_path) -> None:
        self.log_file_path = log_file_path
        self.time_start = None
        self.time_end = None
        self.total_batches = None
        self.time_current_batch_start = None
        self.batch_times = []  # in seconds rounded to one decimal
        self.batch_sizes = []  # in bytes

        # It might be unlikely that client code starts more than
        # one instance of RetrieverLogger.
        # However, as getLogger('retriever') always returns the same logger,
        # handlers added here will be added for every RetrieverLogger
        # instance, resulting in duplicate handlers.
        #
        # By a) first removing all handlers in this __init__ call
        # and b) instantiating RetrieverLogger objects only inside
        # an active Retriever.retrieve_all() (or similar method) call,
        # we can try to avoid the situation where more than one
        # RetrieverLogger is trying to access the same global
        # 'retriever' logger simultaneously.
        #
        # I am unsure if this is the best solution,
        # so this could be a target for refactoring.
        #
        # Note: logger.handlers (list) is technically not documented.
        # However, it appears to be used by others such as:
        # (1) Answer by leoluk https://stackoverflow.com/questions/3630774/
        #   logging-remove-inspect-modify-handlers-configured-by-fileconfig
        # (2) Answer by sfinkens https://stackoverflow.com/questions/19617355/
        #   dynamically-changing-log-level-without-restarting-the-application
        # The discussion in ref 1 notes that root level logger.handlers
        # might get altered by testing, so getting a named
        # logger (rather than the root logger) should be preferred.
        logger = logging.getLogger('retriever')
        logger.setLevel(logging.INFO)
        for handler in logger.handlers.copy():
            logger.removeHandler(handler)
        formatter = logging.Formatter(
            fmt="%(asctime)s | %(levelname)s: %(message)s"
            )
        console_logging = logging.StreamHandler(stream=sys.stdout)
        console_logging.setLevel(logging.INFO)
        console_logging.setFormatter(formatter)
        logger.addHandler(console_logging)
        file_logging = logging.FileHandler(self.log_file_path)
        file_logging.setLevel(logging.INFO)
        file_logging.setFormatter(formatter)
        logger.addHandler(file_logging)
        self.logger = logger

    def log_run_start(self) -> None:
        """Run at the start of a retrieval run to set up self.time_start."""
        self.time_start = time()
        self.logger.info("***STARTING RETRIEVER RUN***")

    def log_resuming_from_file(self) -> None:
        """Log when resuming from existing progress file."""
        self.logger.info("Resuming from existing progress file.")

    def log_new_progress_file(self) -> None:
        """Log when creating a new progress file."""
        self.logger.info("Creating new progress file.")

    def log_total_batches(self, progress: list[dict]) -> None:
        """Log total batches progress file created/loaded."

        Args:
            progress (dict): a progress object, which is a list of dicts
                from Retriever._create_progress_object()
        """
        self.total_batches = len(progress)
        self.logger.info(f"Starting run of {self.total_batches} batches.")

    def log_batch_start(self, idx):
        """Log at the start of a batch request attempt."""
        self.time_current_batch_start = time()
        message = f"- Attempting batch {idx+1} of {self.total_batches}..."
        self.logger.info(message)

    def log_batch_downloaded(
            self,
            idx: int,
            r: requests.Response,
            batch_cooldown: int) -> None:
        """Log upon successful batch data download.

        Args:
            idx (int): Batch index.
            r (requests.Response): Request response for that batch.
            batch_cooldown (int): Upcoming batch cooldown duration in seconds.
        """
        # Batch number
        batch_n = idx + 1
        # Time in seconds
        batch_time = round(time() - self.time_current_batch_start, 1)
        # Size in bytes
        batch_size = len(r.content)
        message = f"--- Batch {batch_n} of {self.total_batches} downloaded"
        message += f" {batch_size/(10**3)} KB"
        message += f" in {batch_time} seconds."
        self.logger.info(message)
        # Update and calculate cumulative times/sizes
        self.batch_times.append(batch_time)
        remaining_batches = self.total_batches - (batch_n)
        time_elapsed = time() - self.time_start
        time_remaining = (median(self.batch_times) + batch_cooldown) \
            * remaining_batches
        self.batch_sizes.append(batch_size)
        cumu_data_size = sum(self.batch_sizes)
        message = f"--- Elapsed: {self._seconds_to_time(time_elapsed)}"
        message += f" | Remaining: {self._seconds_to_time(time_remaining)}"
        self.logger.info(message)
        message = "--- Cumulative data size: "
        message += f"{round(cumu_data_size/(10**6), 1)} MB."
        self.logger.info(message)

    def log_batch_queued(self, idx: int) -> None:
        """Log when a batch is queued on the server.

        Args:
            idx (int): Batch index.
        """
        batch_n = idx + 1
        self.logger.info(
            f"Batch {batch_n} queued at server for later download."
            )

    def log_batch_error(
            self,
            idx: int,
            r: requests.Response) -> None:
        """Log when server gives an error code in the response.

        Prints the code and the text of the response message.

        Args:
            idx (int): Batch index.
            r (requests.Response): Request response for that batch
        """
        self.logger.warning(f"Response with error code {r.status_code}.")
        self.logger.warning("Response text follows:")
        self.logger.warning(f"{r.text}")

    def log_cooldown_start(
            self,
            period: int,
            type: str) -> None:
        """Log the start of a cooldown period.

        Args:
            period (int): Seconds for the cooldown.
            type (str): Type of cooldown (e.g. 'batch', 'server') that gets
                included in the log message.
        """
        period = int(period)
        self.logger.info(f"Starting {type} cooldown of {period} seconds.")

    def log_run_complete_summary(
            self,
            progress: list,
            status_key: str,
            statuses: list) -> None:
        """Log summary upon completion of a retrieval run.

        Args:
            progress (dict): A progress object from
                Retriever._create_progress_object.
            status_key (str): Field key for progress object.
                batch statuses i.e. Retriever.PROGRESS_KEY_STATUS
            statuses (list): List of statuses to be tallied.
                i.e. Retriever.PROGRESS* constants.
        """
        # Total time elapsed
        self.time_end = time()
        elapsed = int(self.time_end - self.time_start)
        message = f"Total elapsed time was {self._seconds_to_time(elapsed)}"
        self.logger.info(message)
        # Batch status summary
        message = f"Total batches was {self.total_batches}."
        self.logger.info(message)
        # Batch status tallies
        tallies = {}
        for status in statuses:
            tallies[status] = 0
        for batch in progress:
            if batch[status_key] in statuses:
                tallies[batch[status_key]] += 1
        for key in tallies.keys():
            message = f"Status '{key}' count: "
            message += f"{tallies[key]}."
            self.logger.info(message)

        self.logger.info(
            "If incomplete and queued batches exist,"
            " run retrieval again with same progress file."
            )
        # Statistics
        message = "Median download times (excluding waits): "
        message += f"{median(self.batch_times)} s."
        self.logger.info(message)
        message = "Total data transferred: "
        message += f"{round(sum(self.batch_sizes)/(10**6), 2)} MB"
        self.logger.info(message)
        self.logger.info("***ENDING RETRIEVER RUN***")

    def _seconds_to_time(self, seconds: Union[int, float]) -> str:
        """Converts number of seconds to str in h m s format."""
        # Coerce int for formatting
        seconds = int(seconds)
        h = seconds // 3600
        m = (seconds % 3600) // 60
        s = seconds % 60
        return f"{h:02}h {m:02}m {s:02}s"
